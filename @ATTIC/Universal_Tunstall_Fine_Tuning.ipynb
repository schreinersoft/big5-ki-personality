{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNoipBsY9PpTygZDv2hKvDB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/schreinersoft/big5-ki-personality/blob/main/Universal_Tunstall_Fine_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fine-Tuning ausgehend von pretrained model"
      ],
      "metadata": {
        "id": "9EOq_8MgpIpx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "AuI9SZbktMIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelckpt = \"distilbert-base-uncased\"\n",
        "modelckpt = \"distilbert-base-cased\"\n",
        "model_name = f\"{modelckpt}-test-finetuned-sms-spam\"\n",
        "dataset_name = \"emotion\"\n",
        "dataset_name = \"ucirvine/sms_spam\"\n",
        "num_labels = 6\n",
        "num_labels = 2"
      ],
      "metadata": {
        "id": "SZBYVSC2tLZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(modelckpt)\n"
      ],
      "metadata": {
        "id": "gVeDZsyM9kYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(dataset_name)\n"
      ],
      "metadata": {
        "id": "_txtcV_xBQP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(batch):\n",
        "  return tokenizer(batch[\"sms\"], padding=True, truncation=True)\n",
        "dataset_encoded = dataset.map(tokenize, batched=True, batch_size=None)"
      ],
      "metadata": {
        "id": "CL3cjFntqEs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the dataset only has a 'train' split\n",
        "if \"train\" in dataset_encoded and len(dataset_encoded) == 1:\n",
        "    dataset_encoded = dataset_encoded[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
        "    dataset_encoded[\"validation\"] = dataset_encoded.pop(\"test\")\n",
        "    print(\"Dataset split into train, validation\")\n",
        "elif \"train\" in dataset_encoded and \"test\" in dataset_encoded:\n",
        "  dataset_encoded[\"validation\"] = dataset_encoded[\"test\"]\n",
        "  print(\"Dataset already has train and test, renamed test to validation\")\n",
        "else:\n",
        "  print(\"Dataset already has train, validation and test splits\")\n",
        "dataset_encoded"
      ],
      "metadata": {
        "id": "AlsBeWUSqO8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_encoded"
      ],
      "metadata": {
        "id": "AE43YEpE1VDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "xInchMpxIAJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Add num_labels to match the number of emotion labels\n",
        "model = AutoModelForSequenceClassification.from_pretrained(modelckpt, num_labels=num_labels).to(device)"
      ],
      "metadata": {
        "id": "g4H6ZHYJIEE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Metriken hinzuf√ºgen"
      ],
      "metadata": {
        "id": "-hPm23xL4ukK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "def computemetrics(pred):\n",
        "  labels = pred.label_ids\n",
        "  preds = pred.predictions.argmax(-1)\n",
        "  f1 = f1_score(labels, preds, average=\"weighted\")\n",
        "  acc = accuracy_score(labels, preds)\n",
        "  return {\"accuracy\": acc, \"f1\": f1}\n"
      ],
      "metadata": {
        "id": "_uNOmNdj4xQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Huggingface einloggen"
      ],
      "metadata": {
        "id": "PCNTE7Tn5RCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "import os\n",
        "notebook_login()\n"
      ],
      "metadata": {
        "id": "BB8KgHRp5NvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "batchsize = 64\n",
        "loggingsteps = len(dataset_encoded[\"train\"]) // batchsize\n",
        "trainingargs = TrainingArguments(\n",
        "    output_dir=model_name,\n",
        "    num_train_epochs=2,\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batchsize,\n",
        "    per_device_eval_batch_size=batchsize,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\",\n",
        "    disable_tqdm=False,\n",
        "    logging_steps=loggingsteps,\n",
        "    push_to_hub=True,\n",
        "    log_level=\"error\"\n",
        "    )\n",
        "wandb_token=\"fa13d32d1d1b21d514ddc7a16dd7b8729598b090\""
      ],
      "metadata": {
        "id": "L6Eu8YgK6Yz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "trainer = Trainer(model=model, args=trainingargs, compute_metrics=computemetrics, train_dataset=dataset_encoded[\"train\"],\n",
        "                  eval_dataset=dataset_encoded[\"validation\"], tokenizer=tokenizer)\n",
        "trainer.train();\n"
      ],
      "metadata": {
        "id": "WhnLWZFn7VMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "R6rsY9e_yCEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predsoutput = trainer.predict(dataset_encoded[\"validation\"])\n"
      ],
      "metadata": {
        "id": "R7Uzs5Dy7DRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predsoutput.metrics"
      ],
      "metadata": {
        "id": "AgDjD21S7HS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "y_preds = np.argmax(predsoutput.predictions, axis=1)\n"
      ],
      "metadata": {
        "id": "knhJWmZz-9mH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Confusion Matrix zur Visualisierung"
      ],
      "metadata": {
        "id": "IKKIB5oM1Ism"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_confusion_matrix(ypreds, ytrue, labels):\n",
        "  cm = confusion_matrix(ytrue, ypreds, normalize=\"true\")\n",
        "  fig, ax = plt.subplots(figsize=(6, 6))\n",
        "  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "  disp.plot(cmap=\"YlOrRd\", values_format=\".2f\", ax=ax, colorbar=False)\n",
        "  plt.title(\"Normalized confusion matrix\")\n",
        "  plt.show()\n",
        "y_valid = dataset_encoded[\"validation\"][\"label\"]\n",
        "labels = dataset_encoded[\"train\"].features[\"label\"].names\n",
        "plot_confusion_matrix(y_preds, y_valid, labels)\n"
      ],
      "metadata": {
        "id": "Q3EQrO5z1Ii4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Error analysis: loss je kategorie ermitteln"
      ],
      "metadata": {
        "id": "ekfukyg3tB9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.functional import cross_entropy\n",
        "def forward_pass_with_label(batch):\n",
        "  # Place all input tensors on the same device as the model\n",
        "  inputs = {k:v.to(device) for k,v in batch.items() if k in tokenizer.model_input_names}\n",
        "  with torch.no_grad():\n",
        "    output = model(**inputs)\n",
        "    pred_label = torch.argmax(output.logits, axis=-1)\n",
        "    loss = cross_entropy(output.logits, batch[\"label\"].to(device), reduction=\"none\")\n",
        "    # Place outputs on CPU for compatibility with other dataset columns\n",
        "  return {\"loss\": loss.cpu().numpy(), \"predictedlabel\": pred_label.cpu().numpy()}\n",
        "\n"
      ],
      "metadata": {
        "id": "PIfkv00x0bv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert our dataset back to PyTorch tensors\n",
        "dataset_encoded.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "# Compute loss values\n",
        "dataset_encoded[\"validation\"] = dataset_encoded[\"validation\"].map(forward_pass_with_label, batched=True, batch_size=16)\n"
      ],
      "metadata": {
        "id": "ephednKe0Fqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label_int2str(row):\n",
        "  return dataset_encoded[\"train\"].features[\"label\"].int2str(row)\n"
      ],
      "metadata": {
        "id": "oWN8rehxvBoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_encoded.set_format(\"pandas\")\n",
        "cols = [\"sms\", \"label\", \"predictedlabel\", \"loss\"]\n",
        "dftest = dataset_encoded[\"validation\"][:][cols]\n",
        "dftest[\"label\"] = dftest[\"label\"].apply(label_int2str)\n",
        "dftest[\"predictedlabel\"] = (dftest[\"predictedlabel\"] .apply(label_int2str))\n"
      ],
      "metadata": {
        "id": "bxBNX95Zt2q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dftest[dftest[\"predictedlabel\"]!=dftest[\"label\"]].sort_values(\"loss\", ascending=True)"
      ],
      "metadata": {
        "id": "XVpNMebOwrZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###schlechteste Vorhersagen suchen und auf eventuelle Fehler im Datensatz pr√ºfen"
      ],
      "metadata": {
        "id": "I_cNGYh2xr1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dftest.sort_values(\"loss\", ascending=False).head(10)"
      ],
      "metadata": {
        "id": "iWb9HybUuRXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Beste Vorhersagen pr√ºfen, z.B. auf Shortcuts (zu einfache Signale im Text)"
      ],
      "metadata": {
        "id": "SvtFW54jx1z1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dftest.sort_values(\"loss\", ascending=True).head(10)\n"
      ],
      "metadata": {
        "id": "NNJYtcqzvJci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dftest[dftest[\"predictedlabel\"]!=dftest[\"label\"]].sort_values(\"loss\", ascending=True).head(10)"
      ],
      "metadata": {
        "id": "B2-Ep8Qwxz0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Auf Huggingface publizieren"
      ],
      "metadata": {
        "id": "9FiYZmO60Q-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub(commit_message=\"Training completed!\")\n"
      ],
      "metadata": {
        "id": "fZL8CvKjzCEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pipeline mit dem eigenen Modell einrichten"
      ],
      "metadata": {
        "id": "VoSb3jz50VMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "modelid = f\"joiner75/{model_name}\"\n",
        "classifier = pipeline(\"text-classification\", model=modelid)\n"
      ],
      "metadata": {
        "id": "xXQmm5x30YBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello Bernd, would you like to meet next sunday?\"\n",
        "text = \"Find all that you need at https://www.goods.com. good prices, big wins\"\n",
        "\n",
        "preds = classifier(text, return_all_scores=True)\n"
      ],
      "metadata": {
        "id": "gSgqyxwH0f6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "predsdf = pd.DataFrame(preds[0])\n",
        "plt.bar(labels, 100 * predsdf[\"score\"], color='C0')\n",
        "plt.title(f'\"{text}\"')\n",
        "plt.ylabel(\"Class probability (%)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "riQTa5GbzZ1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predsdf"
      ],
      "metadata": {
        "id": "_cixYJqTz9zp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}